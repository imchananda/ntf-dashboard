# """
# Y UNIVERSE AWARDS 2025 - Auto Scraper with GitHub Push
# """

# import requests
# from bs4 import BeautifulSoup
# import json
# import re
# from datetime import datetime
# from pathlib import Path
# import subprocess
# import time
# import schedule

# # ========== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ==========
# CONFIG = {
#     'username': 'chan.kwoninine@gmail.com',           # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô email ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
#     'password': '‡∏ò‡∏ü‡∏≥‡πÇ‡∏ü‡∏∑‡∏±‡πÖ‡πÖ‡∏ï/',        # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô password ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
#     'login_url': 'https://acmonlinebiz.com/yna2025/login_action.php',
#     'login_page': 'https://acmonlinebiz.com/yna2025/login.php',
#     'data_url': 'https://acmonlinebiz.com/yna2025/votesummary.php?tpid=4',
#     'interval_hours': 1,
# }

# DATA_DIR = Path('data_yna2025')
# DATA_DIR.mkdir(exist_ok=True)

# class VoteScraper:
#     def __init__(self):
#         self.session = requests.Session()
#         self.is_logged_in = False
#         self.session.headers.update({
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
#         })
    
#     def login(self):
#         try:
#             print("üîê ‡∏Å‡∏≥‡∏•‡∏±‡∏á login...")
#             self.session.get(CONFIG['login_page'], timeout=30)
#             payload = {'username': CONFIG['username'], 'userpassword': CONFIG['password']}
#             response = self.session.post(CONFIG['login_url'], data=payload, timeout=30, allow_redirects=True)
#             if 'login' not in response.url.lower() or self.session.cookies.get_dict():
#                 self.is_logged_in = True
#                 print("‚úÖ Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
#                 return True
#             print("‚ùå Login ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
#             return False
#         except Exception as e:
#             print(f"‚ùå Login error: {e}")
#             return False
    
#     def fetch_data(self):
#         if not self.is_logged_in and not self.login():
#             return None
#         try:
#             response = self.session.get(CONFIG['data_url'], timeout=30)
#             if 'login' in response.url.lower():
#                 self.is_logged_in = False
#                 if not self.login():
#                     return None
#                 response = self.session.get(CONFIG['data_url'], timeout=30)
            
#             html = response.text
#             soup = BeautifulSoup(html, 'html.parser')
            
#             vote_data = {'labels': [], 'data': []}
#             for script in soup.find_all('script'):
#                 if script.string and 'Chart' in script.string:
#                     labels_match = re.search(r'labels\s*:\s*\[(.*?)\]', script.string, re.DOTALL)
#                     if labels_match:
#                         vote_data['labels'] = re.findall(r'["\']([^"\']+)["\']', labels_match.group(1))
#                     data_match = re.search(r"data\s*:\s*\[(.*?)\]", script.string, re.DOTALL)
#                     if data_match:
#                         numbers = re.findall(r'["\']?([\d.]+)["\']?', data_match.group(1))
#                         vote_data['data'] = [float(x) for x in numbers if x]
#                     break
            
#             couples = {}
#             for match in re.finditer(r'(YND\d+)\s*:\s*([^\n]+)', soup.get_text()):
#                 code, rest = match.group(1), match.group(2).strip()
#                 series_match = re.search(r'\(([^)]+)\)\s*$', rest)
#                 couples[code] = {
#                     'names': rest[:series_match.start()].strip() if series_match else rest,
#                     'series': series_match.group(1) if series_match else ''
#                 }
            
#             result = {'timestamp': datetime.now().isoformat(), 'votes': vote_data, 'couples': couples, 'summary': []}
#             for i, label in enumerate(vote_data['labels']):
#                 couple_info = couples.get(label, {})
#                 result['summary'].append({
#                     'code': label,
#                     'percentage': vote_data['data'][i] if i < len(vote_data['data']) else 0,
#                     'names': couple_info.get('names', ''),
#                     'series': couple_info.get('series', ''),
#                 })
#             result['summary'].sort(key=lambda x: x['percentage'], reverse=True)
#             print("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
#             return result
#         except Exception as e:
#             print(f"‚ùå Error: {e}")
#             return None
    
#     def save_data(self, data):
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#         json_file = DATA_DIR / f"vote_{timestamp}.json"
#         with open(json_file, 'w', encoding='utf-8') as f:
#             json.dump(data, f, ensure_ascii=False, indent=2)
#         print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {json_file.name}")
#         return str(json_file)

# def git_push():
#     try:
#         print("üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub...")
#         subprocess.run(['git', 'add', 'data_yna2025/'], check=True, capture_output=True)
#         timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
#         result = subprocess.run(['git', 'commit', '-m', f'Update vote data - {timestamp}'], capture_output=True, text=True)
#         if 'nothing to commit' in result.stdout + result.stderr:
#             print("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á")
#             return True
#         subprocess.run(['git', 'push'], check=True, capture_output=True)
#         print("‚úÖ Push ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
#         return True
#     except Exception as e:
#         print(f"‚ùå Git error: {e}")
#         return False

# def run_job():
#     print(f"\n{'='*50}")
#     print(f"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
#     print('='*50)
#     scraper = VoteScraper()
#     data = scraper.fetch_data()
#     if data:
#         scraper.save_data(data)
#         print("\nüìä ‡∏ú‡∏•‡πÇ‡∏´‡∏ß‡∏ï‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:")
#         for item in data['summary'][:3]:
#             print(f"   {item['code']}: {item['percentage']:.2f}%")
#         git_push()
#     print(f"\n‚è∞ ‡∏£‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô {CONFIG['interval_hours']} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á...")

# if __name__ == '__main__':
#     print("="*50)
#     print("üöÄ Y UNIVERSE AWARDS 2025 - Auto Scraper")
#     print("="*50)
#     run_job()
#     schedule.every(CONFIG['interval_hours']).hours.do(run_job)
#     while True:
#         schedule.run_pending()
#         time.sleep(60)

"""
Y UNIVERSE AWARDS 2025 - Auto Scraper with GitHub Push
‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å 1 ‡∏ä‡∏°. ‡πÅ‡∏•‡πâ‡∏ß push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from pathlib import Path
import subprocess
import time
import schedule

# ========== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ==========
CONFIG = {
    'username': 'YOUR_EMAIL',           # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô email ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    'password': 'YOUR_PASSWORD',        # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô password ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    'login_url': 'https://acmonlinebiz.com/yna2025/login_action.php',
    'login_page': 'https://acmonlinebiz.com/yna2025/login.php',
    'data_url': 'https://acmonlinebiz.com/yna2025/votesummary.php?tpid=4',
    'interval_hours': 1,                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏Å‡∏µ‡πà‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
}

DATA_DIR = Path('data_yna2025')
DATA_DIR.mkdir(exist_ok=True)

# ========== Scraper ==========
class VoteScraper:
    def __init__(self):
        self.session = requests.Session()
        self.is_logged_in = False
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })
    
    def login(self) -> bool:
        try:
            print("üîê ‡∏Å‡∏≥‡∏•‡∏±‡∏á login...")
            self.session.get(CONFIG['login_page'], timeout=30)
            
            payload = {
                'username': CONFIG['username'],
                'userpassword': CONFIG['password'],
            }
            
            response = self.session.post(CONFIG['login_url'], data=payload, timeout=30, allow_redirects=True)
            
            if 'login' not in response.url.lower() or self.session.cookies.get_dict():
                self.is_logged_in = True
                print("‚úÖ Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return True
            
            print("‚ùå Login ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return False
            
        except Exception as e:
            print(f"‚ùå Login error: {e}")
            return False
    
    def fetch_data(self) -> dict | None:
        if not self.is_logged_in:
            if not self.login():
                return None
        
        try:
            response = self.session.get(CONFIG['data_url'], timeout=30)
            
            if 'login' in response.url.lower():
                self.is_logged_in = False
                if not self.login():
                    return None
                response = self.session.get(CONFIG['data_url'], timeout=30)
            
            html = response.text
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract Chart.js data
            vote_data = {'labels': [], 'data': []}
            for script in soup.find_all('script'):
                if script.string and 'Chart' in script.string:
                    labels_match = re.search(r'labels\s*:\s*\[(.*?)\]', script.string, re.DOTALL)
                    if labels_match:
                        vote_data['labels'] = re.findall(r'["\']([^"\']+)["\']', labels_match.group(1))
                    
                    data_match = re.search(r"data\s*:\s*\[(.*?)\]", script.string, re.DOTALL)
                    if data_match:
                        numbers = re.findall(r'["\']?([\d.]+)["\']?', data_match.group(1))
                        vote_data['data'] = [float(x) for x in numbers if x]
                    break
            
            # Extract couple names
            couples = {}
            text = soup.get_text()
            for match in re.finditer(r'(YND\d+)\s*:\s*([^\n]+)', text):
                code = match.group(1)
                rest = match.group(2).strip()
                series_match = re.search(r'\(([^)]+)\)\s*$', rest)
                if series_match:
                    series = series_match.group(1)
                    names = rest[:series_match.start()].strip()
                else:
                    series = ''
                    names = rest
                couples[code] = {'names': names, 'series': series}
            
            # Build result
            result = {
                'timestamp': datetime.now().isoformat(),
                'votes': vote_data,
                'couples': couples,
                'summary': []
            }
            
            for i, label in enumerate(vote_data['labels']):
                couple_info = couples.get(label, {})
                result['summary'].append({
                    'code': label,
                    'percentage': vote_data['data'][i] if i < len(vote_data['data']) else 0,
                    'names': couple_info.get('names', ''),
                    'series': couple_info.get('series', ''),
                })
            
            result['summary'].sort(key=lambda x: x['percentage'], reverse=True)
            
            print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            return result
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
    
    def save_data(self, data: dict) -> str:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = DATA_DIR / f"vote_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {json_file.name}")
        return str(json_file)


# ========== Git Push ==========
def git_push():
    """Push ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô GitHub"""
    try:
        print("üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub...")
        
        # Add all changes
        subprocess.run(['git', 'add', 'data_yna2025/'], check=True, capture_output=True)
        
        # Commit
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        result = subprocess.run(
            ['git', 'commit', '-m', f'Update vote data - {timestamp}'],
            capture_output=True,
            text=True
        )
        
        if 'nothing to commit' in result.stdout or 'nothing to commit' in result.stderr:
            print("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á")
            return True
        
        # Push
        push_result = subprocess.run(['git', 'push'], capture_output=True, text=True)
        
        if push_result.returncode != 0:
            print(f"‚ùå Push error: {push_result.stderr}")
            return False
        
        print("‚úÖ Push ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Git error: {e}")
        print(f"   stderr: {e.stderr}")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False


# ========== Main Job ==========
def run_job():
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub"""
    print(f"\n{'='*50}")
    print(f"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print('='*50)
    
    scraper = VoteScraper()
    data = scraper.fetch_data()
    
    if data:
        scraper.save_data(data)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÇ‡∏´‡∏ß‡∏ï‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        print("\nüìä ‡∏ú‡∏•‡πÇ‡∏´‡∏ß‡∏ï‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:")
        for item in data['summary'][:3]:
            print(f"   {item['code']}: {item['percentage']:.2f}%")
        
        # Push ‡∏Ç‡∏∂‡πâ‡∏ô GitHub
        git_push()
    else:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")
    
    print(f"\n‚è∞ ‡∏£‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÉ‡∏ô {CONFIG['interval_hours']} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á...")


# ========== Main ==========
if __name__ == '__main__':
    print("="*50)
    print("üöÄ Y UNIVERSE AWARDS 2025 - Auto Scraper")
    print("="*50)
    print(f"üìÅ ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {DATA_DIR.absolute()}")
    print(f"‚è∞ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å: {CONFIG['interval_hours']} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á")
    print("‚ùå ‡∏´‡∏¢‡∏∏‡∏î: ‡∏Å‡∏î Ctrl+C")
    print("="*50)
    
    # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    run_job()
    
    # ‡∏ï‡∏±‡πâ‡∏á schedule
    schedule.every(CONFIG['interval_hours']).hours.do(run_job)
    
    # Loop
    while True:
        schedule.run_pending()
        time.sleep(60)