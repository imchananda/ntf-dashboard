"""
‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á Login (Session-based)
- Login ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö session/cookies
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON/CSV
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import os
import logging
from datetime import datetime
from pathlib import Path
import schedule
import time
import hashlib

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class WebScraper:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á login"""
    
    def __init__(self, config: dict):
        """
        Args:
            config: dictionary ‡∏ó‡∏µ‡πà‡∏°‡∏µ keys:
                - login_url: URL ‡∏´‡∏ô‡πâ‡∏≤ login
                - data_url: URL ‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á
                - username: ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
                - password: ‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô
                - username_field: ‡∏ä‡∏∑‡πà‡∏≠ field username ‡πÉ‡∏ô form (default: 'username')
                - password_field: ‡∏ä‡∏∑‡πà‡∏≠ field password ‡πÉ‡∏ô form (default: 'password')
                - output_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (default: 'data')
        """
        self.config = config
        self.session = requests.Session()
        self.is_logged_in = False
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ headers ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô browser ‡∏à‡∏£‡∏¥‡∏á
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'th,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.output_dir = Path(config.get('output_dir', 'data'))
        self.output_dir.mkdir(exist_ok=True)
        
    def get_csrf_token(self, url: str = None) -> dict:
        """‡∏î‡∏∂‡∏á CSRF token ‡πÅ‡∏•‡∏∞ hidden fields ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ login"""
        try:
            # ‡πÉ‡∏ä‡πâ login_page ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á form) ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ url ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
            page_url = url or self.config.get('login_page') or self.config['login_url']
            
            response = self.session.get(page_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            hidden_fields = {}
            
            # ‡∏´‡∏≤ hidden inputs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏ß‡∏° CSRF token)
            for hidden in soup.find_all('input', type='hidden'):
                name = hidden.get('name')
                value = hidden.get('value', '')
                if name:
                    hidden_fields[name] = value
                    
            # ‡∏´‡∏≤ CSRF token ‡∏à‡∏≤‡∏Å meta tag (‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ)
            csrf_meta = soup.find('meta', {'name': ['csrf-token', '_token', 'csrf_token']})
            if csrf_meta:
                hidden_fields['_token'] = csrf_meta.get('content', '')
                
            logger.info(f"‡∏û‡∏ö hidden fields: {list(hidden_fields.keys())}")
            return hidden_fields
            
        except Exception as e:
            logger.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á CSRF token: {e}")
            return {}
    
    def login(self) -> bool:
        """Login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö"""
        try:
            # login_url = URL ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á POST (form action)
            # login_page = URL ‡∏´‡∏ô‡πâ‡∏≤ login ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á form (‡∏ñ‡πâ‡∏≤‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô)
            login_url = self.config['login_url']
            login_page = self.config.get('login_page', login_url)
            
            # ‡∏î‡∏∂‡∏á CSRF token ‡πÅ‡∏•‡∏∞ hidden fields ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ login ‡∏Å‡πà‡∏≠‡∏ô
            hidden_fields = self.get_csrf_token(login_page)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á payload ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö login
            payload = {
                self.config.get('username_field', 'username'): self.config['username'],
                self.config.get('password_field', 'password'): self.config['password'],
                **hidden_fields  # ‡∏£‡∏ß‡∏° hidden fields ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            }
            
            # ‡∏™‡πà‡∏á POST request ‡πÄ‡∏û‡∏∑‡πà‡∏≠ login
            response = self.session.post(
                login_url,
                data=payload,
                timeout=30,
                allow_redirects=True
            )
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if self._check_login_success(response):
                self.is_logged_in = True
                logger.info("‚úÖ Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                return True
            else:
                logger.error("‚ùå Login ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö username/password")
                return False
                
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ login: {e}")
            return False
    
    def _check_login_success(self, response) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö
        
        login_url = self.config['login_url']
        login_page = self.config.get('login_page', login_url)
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL (‡∏ñ‡πâ‡∏≤ redirect ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ login = ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
        if login_url not in response.url and login_page not in response.url:
            return True
            
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cookies
        if 'session' in self.session.cookies.get_dict() or \
           'PHPSESSID' in self.session.cookies.get_dict() or \
           'laravel_session' in self.session.cookies.get_dict():
            return True
            
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° error)
        soup = BeautifulSoup(response.text, 'html.parser')
        error_keywords = ['‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', 'incorrect', 'invalid', 'error', 'failed', '‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á']
        page_text = soup.get_text().lower()
        
        for keyword in error_keywords:
            if keyword in page_text:
                return False
                
        return True
    
    def fetch_data(self) -> dict | None:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"""
        if not self.is_logged_in:
            if not self.login():
                return None
        
        try:
            data_url = self.config['data_url']
            response = self.session.get(data_url, timeout=30)
            
            # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏π‡∏Å redirect ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ login = session ‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏
            login_page = self.config.get('login_page', self.config['login_url'])
            if self.config['login_url'] in response.url or login_page in response.url:
                logger.warning("‚ö†Ô∏è Session ‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏ - ‡∏Å‡∏≥‡∏•‡∏±‡∏á login ‡πÉ‡∏´‡∏°‡πà...")
                self.is_logged_in = False
                if self.login():
                    response = self.session.get(data_url, timeout=30)
                else:
                    return None
            
            response.raise_for_status()
            
            # Parse HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° selector ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            data = self._extract_data(soup, response.text)
            
            logger.info(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏à‡∏≤‡∏Å {data_url}")
            return data
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
            return None
    
    def _extract_data(self, soup: BeautifulSoup, raw_html: str) -> dict:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å HTML - ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'url': self.config['data_url'],
        }
        
        # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î CSS selector ‡∏°‡∏≤
        if 'selectors' in self.config:
            for name, selector in self.config['selectors'].items():
                elements = soup.select(selector)
                if elements:
                    if len(elements) == 1:
                        data[name] = elements[0].get_text(strip=True)
                    else:
                        data[name] = [el.get_text(strip=True) for el in elements]
        
        # ‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        tables = soup.find_all('table')
        if tables:
            data['tables'] = []
            for i, table in enumerate(tables):
                table_data = self._parse_table(table)
                if table_data:
                    data['tables'].append({
                        'table_index': i,
                        'rows': table_data
                    })
        
        # ‡πÄ‡∏Å‡πá‡∏ö raw HTML ‡∏î‡πâ‡∏ß‡∏¢ (optional)
        if self.config.get('save_raw_html', False):
            data['raw_html'] = raw_html
            
        return data
    
    def _parse_table(self, table) -> list:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á HTML ‡πÄ‡∏õ‡πá‡∏ô list of dicts"""
        rows = []
        headers = []
        
        # ‡∏´‡∏≤ headers
        header_row = table.find('thead') or table.find('tr')
        if header_row:
            headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
        
        # ‡∏´‡∏≤ data rows
        tbody = table.find('tbody') or table
        for tr in tbody.find_all('tr')[1:] if not table.find('thead') else tbody.find_all('tr'):
            cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
            if cells and len(cells) == len(headers):
                rows.append(dict(zip(headers, cells)))
            elif cells:
                rows.append(cells)
                
        return rows
    
    def save_data(self, data: dict, format: str = 'json') -> str:
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format == 'json':
            filename = self.output_dir / f"data_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        elif format == 'csv':
            filename = self.output_dir / f"data_{timestamp}.csv"
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏£‡∏Å
            if 'tables' in data and data['tables']:
                rows = data['tables'][0].get('rows', [])
                if rows and isinstance(rows[0], dict):
                    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
                        writer.writeheader()
                        writer.writerows(rows)
            else:
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô flat data
                with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=data.keys())
                    writer.writeheader()
                    writer.writerow(data)
        
        logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà: {filename}")
        return str(filename)
    
    def run_once(self) -> bool:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
        data = self.fetch_data()
        if data:
            self.save_data(data, self.config.get('output_format', 'json'))
            return True
        return False
    
    def run_scheduled(self, interval_hours: int = 1):
        """‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö schedule ‡∏ó‡∏∏‡∏Å X ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á"""
        logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å {interval_hours} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á")
        
        # ‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        self.run_once()
        
        # ‡∏ï‡∏±‡πâ‡∏á schedule
        schedule.every(interval_hours).hours.do(self.run_once)
        
        # Loop ‡∏£‡∏±‡∏ô schedule
        while True:
            schedule.run_pending()
            time.sleep(60)  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏ô‡∏≤‡∏ó‡∏µ


# ============================================
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# ============================================

if __name__ == "__main__":
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á config - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    config = {
        # URLs
        'login_url': 'https://example.com/login',      # URL ‡∏´‡∏ô‡πâ‡∏≤ login
        'data_url': 'https://example.com/dashboard',   # URL ‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        
        # Credentials
        'username': 'your_username',
        'password': 'your_password',
        
        # Field names (‡∏î‡∏π‡∏à‡∏≤‡∏Å HTML form)
        'username_field': 'username',  # ‡∏´‡∏£‡∏∑‡∏≠ 'email', 'user', etc.
        'password_field': 'password',  # ‡∏´‡∏£‡∏∑‡∏≠ 'pass', 'pwd', etc.
        
        # CSS Selectors ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (optional)
        'selectors': {
            'title': 'h1.page-title',
            'stats': '.stat-value',
            'items': '.item-list li',
        },
        
        # Output settings
        'output_dir': 'data',
        'output_format': 'json',  # 'json' ‡∏´‡∏£‡∏∑‡∏≠ 'csv'
        'save_raw_html': False,
    }
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scraper
    scraper = WebScraper(config)
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:
    
    # ‡πÇ‡∏´‡∏°‡∏î 1: ‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
    # scraper.run_once()
    
    # ‡πÇ‡∏´‡∏°‡∏î 2: ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
    scraper.run_scheduled(interval_hours=1)